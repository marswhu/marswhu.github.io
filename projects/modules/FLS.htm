<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="author" content="MARS">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name=keywords content="Mang Ye" , "Ye Mang" , "叶茫" , "WHU" , "Wuhan University" , "武汉大学" , "MARS" , "marswhu"
        , "MARS WHU">

    <title>Projects</title>

    <link href="../../static/bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="../../static/xin.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

</head>

<body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <span class="navbar-brand">
                    <font color="#ffffff">MARS</a></font>
                </span>
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav navbar-nav">
                    <li><a href="../../index.html">Home</a></li>
                    <li><a href="../../publications/index.htm"> Publications </a></li>
                    <li class="active"><a href="../index.htm"> Projects </a></li>
                    <li><a href="../../team/index.htm">Team</a></li>
                    <li><a href="../../teaching/index.htm">Teaching</a></li>
                    <li><a href="../../service/index.htm">Service</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container" style="margin-top: 50px;">
        <!-- <p>
            <br />
            <i class="bi-arrow-left-circle"></i><small><a href="#">Back</a></small>
        </p> -->
        <h2>Federated Learning</h2>
        <div class="row-project">
            <div class="span12">
                <section>
                    <p>Federated Learning enables decentralized training of machine learning models by allowing multiple
                        devices or institutions to collaboratively learn a shared model without sharing their raw data.
                        It is valuable in privacy-sensitive applications, as it preserves data privacy while leveraging
                        distributed datasets for more robust model training.
                    </p>
                    <p>We focus on topics about Federated Learning Security, Generalization and Robustness, and
                        Federated Graph Learning.</p>
                </section>
            </div>
        </div>

        <p>
            <i class="bi-box-arrow-in-left"></i><small><a style="text-decoration: none;" href="javascript:history.back(-1)">
                    <b>Back</b> </a></small>
        </p>

        <div class="page-header">
            <h3><i>Highlight: Survey and Benchmarks</i></h3>
        </div>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/TPAMI24_FL_Survey.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Federated learning for generalization, robustness, fairness: A
                                survey and benchmark</strong></div>
                        <div class="pub-authors">Wenke Huang, <u>Mang Ye*</u>, Zekun Shi, Guancheng Wan, He Li, Bo Du,
                            Qiang Yang</div>
                        <div class="pub-venue"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence
                                (TPAMI), 2024</i>
                        </div>

                        <br>
                        We comprehensively
                        review three basic lines of research:
                        generalization, robustness, and fairness, by introducing their respective background concepts,
                        task settings, and main challenges. We
                        also offer a detailed overview of representative literature on both methods and datasets. We
                        further benchmark the reviewed methods
                        on several well-known datasets. Finally, we point out several open issues in this field and
                        suggest opportunities for further research.

                        <div class="pub-description">
                            <a href="https://arxiv.org/pdf/2311.06750" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/WenkeHuang/MarsFL" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/CSUR23_HFL_Survey.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Heterogeneous Federated Learning: State-of-the-art and Research
                                Challenges</strong></div>
                        <div class="pub-authors"><u>Mang Ye*</u>, Xiuwen Fang, Bo Du, Pong C Yuen, Dachen Tao </div>
                        <div class="pub-venue"><i>ACM Computing Surveys, 2023.</i>
                        </div>

                        <br>
                        We firstly summarize the various research challenges in
                        HFL from five aspects: statistical heterogeneity, model heterogeneity, communication
                        heterogeneity, device heterogeneity, and additional
                        challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL
                        methods is proposed with an in-depth
                        analysis of their pros and cons. We classify existing methods from three different levels
                        according to the HFL procedure: data-level,
                        model-level, and server-level. Finally, several critical and promising future research
                        directions in HFL are discussed, which may facilitate
                        further developments in this field.

                        <div class="pub-description">
                            <a href="https://arxiv.org/pdf/2307.10616.pdf" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/marswhu/HFL_Survey" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/VFL_Survey.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Vertical Federated Learning for Effectiveness, Security,
                                Applicability: A Survey</strong></div>
                        <div class="pub-authors"><u>Mang Ye*</u>, Wei Shen, Bo Du, Eduard Snezhko, Vassili Kovalev, Pong
                            C. Yuen</div>
                        <div class="pub-venue"><i>preprint.</i></div>

                        <br>
                        We provide a history and background introduction, along with a summary of the general training
                        protocol of VFL. We then revisit the taxonomy in recent reviews and analyze limitations
                        in-depth. For a comprehensive and structured discussion, we synthesize recent research from
                        three fundamental perspectives: effectiveness, security, and applicability. Finally, we discuss
                        several critical future research directions in VFL, which will facilitate the developments in
                        this field.

                        <div class="pub-description">
                            <a href="https://arxiv.org/pdf/2405.17495" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/shentt67/VFL_Survey" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="page-header">
            <h3><i>Highlight: Federated Learning Security</i></h3>
        </div>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/NeurIPS24_FDCR.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Parameter Disparities Dissection for Backdoor Defense in
                                Heterogeneous Federated Learning</strong></div>
                        <div class="pub-authors">Wenke Huang, <u>Mang Ye*</u>, Zekun Shi, Guancheng Wan, He Li*, Bo Du
                        </div>
                        <div class="pub-venue"><i>Thirty-seventh Conference on Neural Information Processing Systems
                                (NeurIPS), 2024.</i>
                        </div>

                        <br>
                        We introduce
                        the Fisher Discrepancy Cluster and Rescale (FDCR) method, which utilizes Fisher
                        Information to calculate the degree of parameter importance for local distributions.
                        This allows us to reweight client parameter updates and identify those with large
                        discrepancies as backdoor attackers. Furthermore, we prioritize rescaling important
                        parameters to expedite adaptation to the target distribution, encouraging significant
                        elements to contribute more while diminishing the influence of trivial ones. This
                        approach enables FDCR to handle backdoor attacks in heterogeneous federated
                        learning environments.

                        <div class="pub-description">
                            <a href="../../publications/files/NeurIPS24_FDCR.pdf" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/wenkehuang/FDCR" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/ICML24_SDEA.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Self-Driven Entropy Aggregation for Byzantine-Robust
                                Heterogeneous Federated Learning</strong></div>
                        <div class="pub-authors"> Wenke Huang<sup>#</sup>, Zekun Shi<sup>#</sup>, <u>Mang Ye*</u>, He
                            Li, Bo Du</div>
                        <div class="pub-venue"><i>International Conference on Machine Learning (ICML), 2024.</i>
                        </div>

                        <br>
                        We propose Self-Driven Entropy Aggregation (SDEA), which leverages the random public dataset to
                        conduct Byzantine-robust aggregation in heterogeneous federated learning. For
                        Byzantine attackers, we observe that benign ones
                        typically present more confident (sharper) predictions than evils on the public dataset. Thus,
                        we highlight benign clients by introducing learnable aggregation weight to minimize the
                        instanceprediction entropy of the global model on the
                        random public dataset. Besides, with inherent
                        data heterogeneity, we reveal that it brings heterogeneous sharpness. Specifically, clients are
                        optimized under distinct distribution and thus present
                        fruitful predictive preferences. The learnable aggregation weight blindly allocates high
                        attention
                        to limited ones for sharper predictions, resulting in a biased global model. To alleviate this
                        problem, we encourage the global model to offer
                        diverse predictions via batch-prediction entropy
                        maximization and conduct clustering to equally
                        divide honest weights to accommodate different
                        tendencies. This endows SDEA to detect Byzantine attackers in heterogeneous federated learning.

                        <div class="pub-description">
                            <a href="https://openreview.net/pdf?id=k2axqNsVVO" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/WenkeHuang/SDEA" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/ECCV24_SDFC.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Fisher Calibration for Backdoor-Robust Heterogeneous Federated
                                Learning</strong></div>
                        <div class="pub-authors"> Wenke Huang, <u>Mang Ye*</u>, Zekun Shi, Bo Du, Dacheng Tao</div>
                        <div class="pub-venue"><i>European Conference on Computer Vision (ECCV), 2024.</i>
                        </div>

                        <br>
                        We propose the Self-Driven Fisher Calibration (SDFC), which utilizes the
                        Fisher Information to calculate the parameter importance degree for the
                        local agnostic and global validation distribution and regulate those elements with large
                        important differences. Furthermore, we allocate high
                        aggregation weight for clients with relatively small overall parameter
                        differences, which encourages clients with close local distribution to the
                        global distribution, to contribute more to the federation. This endows
                        SDFC to handle backdoor attackers in heterogeneous federated learning.

                        <div class="pub-description">
                            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02353.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/WenkeHuang/SDFC" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/NeurIPS23_FedDPA.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Dynamic Personalized Federated Learning with Adaptive
                                Differential Privacy</strong></div>
                        <div class="pub-authors">Xiyuan Yang<sup>#</sup>, Wenke Huang<sup>#</sup>, <u>Mang Ye*</u></div>
                        <div class="pub-venue"><i>Thirty-seventh Conference on Neural Information Processing Systems
                                (NeurIPS), 2023.</i>
                        </div>

                        <br>
                        We propose a novel federated learning
                        method with Dynamic Fisher Personalization and Adaptive Constraint (FedDPA) to
                        handle these challenges. Firstly, by using layer-wise Fisher information to measure
                        the information content of local parameters, we retain local parameters with high
                        Fisher values during the personalization process, which are considered informative,
                        simultaneously prevent these parameters from noise perturbation. Secondly, we
                        introduce an adaptive approach by applying differential constraint strategies to
                        personalized parameters and shared parameters identified in the previous for better
                        convergence.

                        <div class="pub-description">
                            <a href="https://papers.neurips.cc/paper_files/paper/2023/file/e4724af0e2a0d52ce5a0a4e084b87f59-Paper-Conference.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/xiyuanyang45/DynamicPFL" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="page-header">
            <h3><i>Highlight: Generalization and Robustness</i></h3>
        </div>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/TPAMI23_FCCL.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Generalizable Heterogeneous Federated Cross-Correlation and
                                Instance Similarity Learning</strong></div>
                        <div class="pub-authors">Wenke Huang, <u>Mang Ye*</u>, Zekun Shi, Bo Du </div>
                        <div class="pub-venue"><i>IEEE Transactions on Pattern Analysis & Machine Intelligence,
                                2023.</i>
                        </div>

                        <br>
                        We presents a novel FCCL+, federated correlation and similarity learning with nontarget
                        distillation, facilitating the both intra-domain discriminability and inter-domain
                        generalization. For heterogeneity issue, we leverage
                        irrelevant unlabeled public data for communication between the heterogeneous participants. We
                        construct cross-correlation matrix
                        and align instance similarity distribution on both logits and feature levels, which effectively
                        overcomes the communication barrier
                        and improves the generalizable ability. For catastrophic forgetting in local updating stage,
                        FCCL+ introduces Federated Non Target
                        Distillation, which retains inter-domain knowledge while avoiding the optimization conflict
                        issue, fulling distilling privileged inter-domain
                        information through depicting posterior classes relation. Considering that there is no standard
                        benchmark for evaluating existing
                        heterogeneous federated learning under the same setting, we present a comprehensive benchmark
                        with extensive representative
                        methods under four domain shift scenarios, supporting both heterogeneous and homogeneous
                        federated settings.

                        <div class="pub-description">
                            <a href="https://arxiv.org/pdf/2309.16286.pdf" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/WenkeHuang/FCCL" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/CVPR24_FedAS.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>FedAS: Bridging Inconsistency in Personalized Federated
                                Learning</strong></div>
                        <div class="pub-authors">Xiyuan Yang, Wenke Huang, <u>Mang Ye*</u></div>
                        <div class="pub-venue"><i>Proceedings of the IEEE/CVF Conference on Computer Vision and
                                Pattern Recognition (CVPR), 2024.</i>
                        </div>

                        <br>
                        We present a novel PFL framework named
                        FedAS, which uses Federated Parameter-Alignment and
                        Client-Synchronization to overcome above challenges. Initially, we enhance the localization
                        of global parameters by
                        infusing them with local insights. We make the shared parts
                        learn from previous model, thereby increasing their local
                        relevance and reducing the impact of parameter inconsistency. Furthermore, we design a
                        robust aggregation method
                        to mitigate the impact of stragglers by preventing the incorporation of their under-trained
                        knowledge into aggregated
                        model.

                        <div class="pub-description">
                            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_FedAS_Bridging_Inconsistency_in_Personalized_Federated_Learning_CVPR_2024_paper.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/xiyuanyang45/FedAS" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/CVPR24_FedHEAL.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Fair Federated Learning under Domain Skew with Local Consistency
                                and Domain Diversity</strong></div>
                        <div class="pub-authors">Yuhang Chen<sup>#</sup>, Wenke Huang<sup>#</sup>, <u>Mang Ye*</u></div>
                        <div class="pub-venue"><i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                Recognition (CVPR), 2024.</i>
                        </div>

                        <br>
                        We selectively discard unimportant parameter updates to prevent updates from clients with lower
                        performance overwhelmed by unimportant parameters, resulting in fairer generalization
                        performance. Second, we propose a fair aggregation objective to prevent global model
                        bias towards some domains, ensuring that the global model
                        continuously aligns with an unbiased model.

                        <div class="pub-description">
                            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Fair_Federated_Learning_under_Domain_Skew_with_Local_Consistency_and_CVPR_2024_paper.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/yuhangchen0/FedHEAL" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/ICCV23_AugHFL.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Robust Heterogeneous Federated Learning under Data
                                Corruption</strong></div>
                        <div class="pub-authors">Xiuwen Fang, <u>Mang Ye*</u>, Xiyuan Yang</div>
                        <div class="pub-venue"><i>International Conference on Computer Vision (ICCV), 2023.</i>
                        </div>

                        <br>
                        We design a novel method named Augmented Heterogeneous Federated Learning (AugHFL), which
                        consists of two stages: (1)
                        In the local update stage, a corruption-robust data augmentation strategy is adopted to minimize
                        the adverse effects of
                        local corruption while enabling the models to learn rich local knowledge. (2) In the
                        collaborative update stage, we design a robust re-weighted communication approach, which
                        implements communication between heterogeneous models
                        while mitigating corrupted knowledge transfer from others.

                        <div class="pub-description">
                            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_Robust_Heterogeneous_Federated_Learning_under_Data_Corruption_ICCV_2023_paper.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/FangXiuwen/AugHFL" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/CVPR23_FPL.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Rethinking Federated Learning With Domain Shift: A Prototype
                                View</strong></div>
                        <div class="pub-authors">Wenke Huang, <u>Mang Ye*</u>, Zekun Shi, He Li, Bo Du</div>
                        <div class="pub-venue"><i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                Recognition (CVPR), 2023.</i>
                        </div>

                        <br>
                        We propose Federated Prototypes Learning (FPL) for federated learning under domain
                        shift. The core idea is to construct cluster prototypes and
                        unbiased prototypes, providing fruitful domain knowledge
                        and a fair convergent target. On the one hand, we pull the
                        sample embedding closer to cluster prototypes belonging
                        to the same semantics than cluster prototypes from distinct
                        classes. On the other hand, we introduce consistency regularization to align the local instance
                        with the respective
                        unbiased prototype.

                        <div class="pub-description">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/WenkeHuang/RethinkFL" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/CVPR22_RHFL.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Robust Federated Learning With Noisy and Heterogeneous
                                Clients</strong></div>
                        <div class="pub-authors">Xiuwen Fang, <u>Mang Ye*</u></div>
                        <div class="pub-venue"><i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                Recognition (CVPR), 2022.</i>
                        </div>

                        <br>
                        We present
                        a novel solution RHFL (Robust Heterogeneous Federated
                        Learning), which simultaneously handles the label noise
                        and performs federated learning in a single framework. It
                        is featured in three aspects: (1) For the communication between heterogeneous models, we
                        directly align the models
                        feedback by utilizing public data, which does not require
                        additional shared global models for collaboration. (2) For
                        internal label noise, we apply a robust noise-tolerant loss
                        function to reduce the negative effects. (3) For challenging
                        noisy feedback from other participants, we design a novel
                        client confidence re-weighting scheme, which adaptively assigns corresponding weights to each
                        client in the collaborative learning stage.

                        <div class="pub-description">
                            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fang_Robust_Federated_Learning_With_Noisy_and_Heterogeneous_Clients_CVPR_2022_paper.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/FangXiuwen/Robust_FL" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="page-header">
            <h3><i>Highlight: Federated Graph Learning</i></h3>
        </div>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/NeurIPS24_FedSSP.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>FedSSP: Federated Graph Learning with Spectral Knowledge and
                                Personalized Preference</strong></div>
                        <div class="pub-authors">Zihan Tan<sup>#</sup>, Guancheng Wan<sup>#</sup>, Wenke
                            Huang<sup>#</sup>, <u>Mang Ye*</u></div>
                        <div class="pub-venue"><i>Thirty-seventh Conference on Neural Information Processing Systems
                                (NeurIPS), 2024.</i>
                        </div>

                        <br>
                        We innovatively reveal that inherent domain structural shift can
                        be well reflected by the spectral nature of graphs. Correspondingly, our method
                        overcomes it by sharing generic spectral knowledge. Moreover, we indicate the
                        biased message-passing schemes for graph structures and propose the personalized
                        preference module. Combining both strategies for effective global collaboration
                        and personalized local application, we propose our pFGL framework FedSSP
                        which Shares generic Spectral knowledge while satisfying graph Preferences.

                        <div class="pub-description">
                            <a href="../../publications/files/NeurIPS24_FedSSP.pdf" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com" target="_blank"><span class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/AAAI24_FGGP.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Federated Graph Learning under Domain Shift with Generalizable
                                Prototypes</strong></div>
                        <div class="pub-authors"> Guancheng Wan, Wenke Huang, <u>Mang Ye*</u></div>
                        <div class="pub-venue"><i>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
                                2024.</i>
                        </div>

                        <br>
                        We
                        propose a novel framework called Federated Graph Learning with Generalizable Prototypes (FGGP).
                        It decouples the
                        global model into two levels and bridges them via prototypes.
                        These prototypes, which are semantic centers derived from
                        the feature extractor, can provide valuable classifcation information. At the classifcation
                        model level, we innovatively
                        eschew the traditional classifers, then instead leverage clustered prototypes to capture
                        fruitful domain information and
                        enhance the discriminative capability of the classes, improving the performance of multi-domain
                        predictions. Furthermore, at the feature extractor level, we go beyond traditional
                        approaches by implicitly injecting distinct global knowledge
                        and employing contrastive learning to obtain more powerful
                        prototypes while enhancing the feature extractor generalization ability.

                        <div class="pub-description">
                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29468/30767" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/GuanchengWan/FGGP" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/FLS/IJCAI23_FGSSL.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">

                    <div class="pub-info">
                        <div class="pub-title"><strong>Federated Graph Semantic and Structural Learning</strong></div>
                        <div class="pub-authors">Wenke Huang<sup>#</sup>, Guancheng Wan<sup>#</sup>, <u>Mang Ye*</u>, Bo
                            Du</div>
                        <div class="pub-venue"><i>International Joint Conference on Artificial Intelligence (IJCAI),
                                2023.</i>
                        </div>

                        <br>
                        We introduce a novel federated graph learning (FGSSL)
                        frame for both node and graph-level calibration. The former Federated Node Semantic Contrast
                        calibrates local
                        node semantics with the assistance of the global model
                        without compromising privacy. The latter Federated Graph
                        Structure Distillation transforms the adjacency relationships from the global model to the local
                        model, fully reinforcing the graph representation with aggregated relation.

                        <div class="pub-description">
                            <a href="../../publications/files/FGSSL.pdf" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/wgc-research/fgssl" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>



        <p></p>
        </section>

        <div align="center">
            <small>Copyright &copy 2024 <a href="https://marswhu.github.io/">Multimedia Analysis & Reasoning (MARS)
                    Lab</a></small>
            <br>
            <small><a href="https://www.whu.edu.cn/">Wuhan University 武汉大学</a></small>
        </div>
    </div>


    <script src="../../static/jquery.js"></script>
    <script src="../../static/bootstrap/js/bootstrap.js"></script>
</body>

</html>